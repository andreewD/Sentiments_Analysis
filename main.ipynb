{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#NLTK\n",
    "import nltk\n",
    "from nltk import word_tokenize, WordPunctTokenizer, regexp_tokenize\n",
    "from nltk import word_tokenize, WordPunctTokenizer, regexp_tokenize\n",
    "\n",
    "#Plotting \n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "#Keras\n",
    "from numpy import array\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "tree = ET.parse('train2.xml')\n",
    "root = tree.getroot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = pd.DataFrame({'tweet_id':[],'tweetText':[],'polarity_value':[],'polarity_type':[],'topic':[]})\n",
    "row=0\n",
    "for tweet in root:\n",
    "    tweet_id = 'ID:'+tweet.find('tweetid').text\n",
    "    tweetText = tweet.find('content').text\n",
    "    lang = tweet.find('lang').text\n",
    "    polarity_value = tweet.find('sentiments').find('polarity').find('value').text\n",
    "    polarity_type = tweet.find('sentiments').find('polarity').find('type').text\n",
    "    topic = tweet.find('topics').find('topic').text\n",
    "    \n",
    "    if lang == 'es':\n",
    "        train_set.loc[row] = [tweet_id,tweetText,polarity_value,polarity_type,topic]\n",
    "        row+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set['set']='train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>tweetText</th>\n",
       "      <th>polarity_value</th>\n",
       "      <th>polarity_type</th>\n",
       "      <th>topic</th>\n",
       "      <th>set</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ID:142389495503925248</td>\n",
       "      <td>Salgo de #VeoTV , que día más largoooooo...</td>\n",
       "      <td>NONE</td>\n",
       "      <td>AGREEMENT</td>\n",
       "      <td>otros</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ID:142389933619945473</td>\n",
       "      <td>@PauladeLasHeras No te libraras de ayudar me/n...</td>\n",
       "      <td>NEU</td>\n",
       "      <td>DISAGREEMENT</td>\n",
       "      <td>otros</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ID:142391947707940864</td>\n",
       "      <td>@marodriguezb Gracias MAR</td>\n",
       "      <td>P</td>\n",
       "      <td>AGREEMENT</td>\n",
       "      <td>otros</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ID:142416095012339712</td>\n",
       "      <td>Off pensando en el regalito Sinde, la que se v...</td>\n",
       "      <td>N+</td>\n",
       "      <td>AGREEMENT</td>\n",
       "      <td>política</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ID:142422495721562112</td>\n",
       "      <td>Conozco a alguien q es adicto al drama! Ja ja ...</td>\n",
       "      <td>P+</td>\n",
       "      <td>AGREEMENT</td>\n",
       "      <td>otros</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ID:142424715175280640</td>\n",
       "      <td>RT @FabHddzC: Si amas a alguien, déjalo libre....</td>\n",
       "      <td>NONE</td>\n",
       "      <td>AGREEMENT</td>\n",
       "      <td>música</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ID:142483342040907776</td>\n",
       "      <td>Toca @crackoviadeTV3 . Grabación dl especial N...</td>\n",
       "      <td>P+</td>\n",
       "      <td>AGREEMENT</td>\n",
       "      <td>entretenimiento</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ID:142493511634259968</td>\n",
       "      <td>Hoy asisitiré en Madrid a un seminario sobre l...</td>\n",
       "      <td>NONE</td>\n",
       "      <td>AGREEMENT</td>\n",
       "      <td>política</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ID:142494476051562496</td>\n",
       "      <td>Buen día todos! Lo primero mandar un abrazo gr...</td>\n",
       "      <td>P+</td>\n",
       "      <td>AGREEMENT</td>\n",
       "      <td>otros</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ID:142496796416016384</td>\n",
       "      <td>Desde el escaño. Todo listo para empezar #endi...</td>\n",
       "      <td>P+</td>\n",
       "      <td>AGREEMENT</td>\n",
       "      <td>política</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                tweet_id                                          tweetText  \\\n",
       "0  ID:142389495503925248        Salgo de #VeoTV , que día más largoooooo...   \n",
       "1  ID:142389933619945473  @PauladeLasHeras No te libraras de ayudar me/n...   \n",
       "2  ID:142391947707940864                          @marodriguezb Gracias MAR   \n",
       "3  ID:142416095012339712  Off pensando en el regalito Sinde, la que se v...   \n",
       "4  ID:142422495721562112  Conozco a alguien q es adicto al drama! Ja ja ...   \n",
       "5  ID:142424715175280640  RT @FabHddzC: Si amas a alguien, déjalo libre....   \n",
       "6  ID:142483342040907776  Toca @crackoviadeTV3 . Grabación dl especial N...   \n",
       "7  ID:142493511634259968  Hoy asisitiré en Madrid a un seminario sobre l...   \n",
       "8  ID:142494476051562496  Buen día todos! Lo primero mandar un abrazo gr...   \n",
       "9  ID:142496796416016384  Desde el escaño. Todo listo para empezar #endi...   \n",
       "\n",
       "  polarity_value polarity_type            topic    set  \n",
       "0           NONE     AGREEMENT            otros  train  \n",
       "1            NEU  DISAGREEMENT            otros  train  \n",
       "2              P     AGREEMENT            otros  train  \n",
       "3             N+     AGREEMENT         política  train  \n",
       "4             P+     AGREEMENT            otros  train  \n",
       "5           NONE     AGREEMENT           música  train  \n",
       "6             P+     AGREEMENT  entretenimiento  train  \n",
       "7           NONE     AGREEMENT         política  train  \n",
       "8             P+     AGREEMENT            otros  train  \n",
       "9             P+     AGREEMENT         política  train  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Mostrando la data\n",
    "train_set.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.columns = ['tweet_id', 'tweetText', 'polarity_value', 'polarity_type', 'topic','set']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>tweetText</th>\n",
       "      <th>polarity_value</th>\n",
       "      <th>polarity_type</th>\n",
       "      <th>topic</th>\n",
       "      <th>set</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ID:142389495503925248</td>\n",
       "      <td>Salgo de #VeoTV , que día más largoooooo...</td>\n",
       "      <td>NONE</td>\n",
       "      <td>AGREEMENT</td>\n",
       "      <td>otros</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ID:142389933619945473</td>\n",
       "      <td>@PauladeLasHeras No te libraras de ayudar me/n...</td>\n",
       "      <td>NEU</td>\n",
       "      <td>DISAGREEMENT</td>\n",
       "      <td>otros</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ID:142391947707940864</td>\n",
       "      <td>@marodriguezb Gracias MAR</td>\n",
       "      <td>P</td>\n",
       "      <td>AGREEMENT</td>\n",
       "      <td>otros</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ID:142416095012339712</td>\n",
       "      <td>Off pensando en el regalito Sinde, la que se v...</td>\n",
       "      <td>N+</td>\n",
       "      <td>AGREEMENT</td>\n",
       "      <td>política</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                tweet_id                                          tweetText  \\\n",
       "0  ID:142389495503925248        Salgo de #VeoTV , que día más largoooooo...   \n",
       "1  ID:142389933619945473  @PauladeLasHeras No te libraras de ayudar me/n...   \n",
       "2  ID:142391947707940864                          @marodriguezb Gracias MAR   \n",
       "3  ID:142416095012339712  Off pensando en el regalito Sinde, la que se v...   \n",
       "\n",
       "  polarity_value polarity_type     topic    set  \n",
       "0           NONE     AGREEMENT     otros  train  \n",
       "1            NEU  DISAGREEMENT     otros  train  \n",
       "2              P     AGREEMENT     otros  train  \n",
       "3             N+     AGREEMENT  política  train  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "política           2715\n",
       "otros              1620\n",
       "entretenimiento    1209\n",
       "economía            525\n",
       "música              412\n",
       "fútbol              225\n",
       "cine                183\n",
       "tecnología          144\n",
       "deportes            101\n",
       "literatura           84\n",
       "Name: topic, dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.topic.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "P+      1652\n",
       "NONE    1482\n",
       "N       1335\n",
       "P       1232\n",
       "N+       847\n",
       "NEU      670\n",
       "Name: polarity_value, dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.polarity_value.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = train_set.loc[(train_set.polarity_value == 'P') | (train_set.polarity_value == 'N') ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweets Peruanos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets=pd.read_json('file.json',lines=True)\n",
    "tweets['set']='test'\n",
    "tweets['polarity_value']=np.NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1026, 38)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Dimensiones\n",
    "tweets.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Procesando Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweetID</th>\n",
       "      <th>tweetText</th>\n",
       "      <th>polarity_value</th>\n",
       "      <th>set</th>\n",
       "      <th>processed_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1562</th>\n",
       "      <td>ID:153901981797335040</td>\n",
       "      <td>Se congelan el sueldo de los funcionarios, per...</td>\n",
       "      <td>N</td>\n",
       "      <td>train</td>\n",
       "      <td>Se congelan el sueldo de los funcionarios, per...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>510</th>\n",
       "      <td>1328024276679749633</td>\n",
       "      <td>SE LOGRO!!!!! RENUNCIO ESA RATA!!!! #MERINONOM...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "      <td>SE LOGRO!!!!! RENUNCIO ESA RATA!!!! #MERINONOM...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1608</th>\n",
       "      <td>ID:154717637895655426</td>\n",
       "      <td>Urdangarin pagó a la pareja rumana q atendía e...</td>\n",
       "      <td>N</td>\n",
       "      <td>train</td>\n",
       "      <td>Urdangarin pagó a la pareja rumana q atendía e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1958</th>\n",
       "      <td>ID:161957420409688064</td>\n",
       "      <td>Levante y Mirandés, dos modestos rompiendo mol...</td>\n",
       "      <td>P</td>\n",
       "      <td>train</td>\n",
       "      <td>Levante y Mirandés, dos modestos rompiendo mol...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    tweetID  \\\n",
       "1562  ID:153901981797335040   \n",
       "510     1328024276679749633   \n",
       "1608  ID:154717637895655426   \n",
       "1958  ID:161957420409688064   \n",
       "\n",
       "                                              tweetText polarity_value    set  \\\n",
       "1562  Se congelan el sueldo de los funcionarios, per...              N  train   \n",
       "510   SE LOGRO!!!!! RENUNCIO ESA RATA!!!! #MERINONOM...            NaN   test   \n",
       "1608  Urdangarin pagó a la pareja rumana q atendía e...              N  train   \n",
       "1958  Levante y Mirandés, dos modestos rompiendo mol...              P  train   \n",
       "\n",
       "                                        processed_tweet  \n",
       "1562  Se congelan el sueldo de los funcionarios, per...  \n",
       "510   SE LOGRO!!!!! RENUNCIO ESA RATA!!!! #MERINONOM...  \n",
       "1608  Urdangarin pagó a la pareja rumana q atendía e...  \n",
       "1958  Levante y Mirandés, dos modestos rompiendo mol...  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_tweets= pd.concat([pd.DataFrame({'tweetID':tweets.id, 'tweetText':tweets.tweet,'polarity_value':tweets.polarity_value, 'set':tweets.set}), \n",
    "                             pd.DataFrame({'tweetID':train_set.tweet_id, 'tweetText':train_set.tweetText, 'polarity_value':train_set.polarity_value,'set':train_set.set})], ignore_index=True)\n",
    "processed_tweets['processed_tweet'] = processed_tweets.tweetText\n",
    "processed_tweets.sample(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "hash_regex = re.compile(r\"#(\\w+)\")\n",
    "hstgs = [] # To store the hashtags so we can exclude them from some parts of the analysis\n",
    "def hash_repl(match):\n",
    "    _ = '__HASH_'+match.group(1).upper()\n",
    "    hstgs.append(_)\n",
    "    return _"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## USERNAMES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_regex = re.compile(r\"@(\\w+)\")\n",
    "usr_names = [] # To store the user names so we can exclude them from some parts of the analysis\n",
    "def user_repl(match):\n",
    "    _ = '__user_'+match.group(1).upper()\n",
    "    usr_names.append(_)\n",
    "    return _"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_regex = re.compile(r\"(http|https|ftp)://[a-zA-Z0-9\\./]+\")\n",
    "def url_repl(match):\n",
    "    return '__URL_'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REPETI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeating words like hurrrryyyyyy\n",
    "rpt_regex = re.compile(r\"(.)\\1{1,}\", re.IGNORECASE);\n",
    "def rpt_repl(match):\n",
    "    return match.group(1)+match.group(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Reppeated characters in wordss'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test\n",
    "re.sub(rpt_regex, rpt_repl, \"Reppppeated characters in wordsssssssss\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Puntuacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spliting by word boundaries\n",
    "word_bound_regex = re.compile(r\"\\W+\")\n",
    "\n",
    "# Punctuations\n",
    "punctuations = \\\n",
    "\t[\t#('',\t\t['.', ] )\t,\\\n",
    "\t\t#('',\t\t[',', ] )\t,\\\n",
    "\t\t#('',\t\t['\\'', '\\\"', ] )\t,\\\n",
    "\t\t('__PUNC_EXCL',\t\t['!', '¡', ] )\t,\\\n",
    "\t\t('__PUNC_QUES',\t\t['?', '¿', ] )\t,\\\n",
    "\t\t('__PUNC_ELLP',\t\t['...', '…', ] )\t,\\\n",
    "\t]\n",
    "\n",
    "#For punctuation replacement\n",
    "def punctuations_repl(match):\n",
    "\ttext = match.group(0)\n",
    "\trepl = []\n",
    "\tfor (key, parr) in punctuations :\n",
    "\t\tfor punc in parr :\n",
    "\t\t\tif punc in text:\n",
    "\t\t\t\trepl.append(key)\n",
    "\tif( len(repl)>0 ) :\n",
    "\t\treturn ' '+' '.join(repl)+' '\n",
    "\telse :\n",
    "\t\treturn ' '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.punctuations_repl(match)>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "stemmer = SnowballStemmer('spanish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@MerinoDeLama Tienes el repudio de gran parte de la población peruana. #merinonomerepresenta\n",
      "------------------\n",
      "@merinodelam tien repudi gran part poblacion peruana. #merinonomerepresent\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "text = processed_tweets.processed_tweet[55]\n",
    "print(text)\n",
    "text = [word if(word[0:2]=='__') else word.lower() for word in text.split() if ((len(word) >= 3) and (word not in usr_names))]\n",
    "text = [stemmer.stem(w) for w in text]                \n",
    "text = \" \".join(text)\n",
    "print('------------------')\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sb_stem(text, only_first=0):\n",
    "    text = [word if(word[0:2]=='__') else word.lower() for word in text.split() if ((len(word) >= 3) or (word in ['no','si', 'sí', 'ni']))] #   If we are doing negation analysis, maybe is a better idea to keep the small words (like 'no')\n",
    "    text = [stemmer.stem(w) if w[0:2]!='__' else w for w in text ]\n",
    "    \n",
    "    return(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def processAll(text):\n",
    "    text = re.sub( hash_regex, hash_repl, text )\n",
    "    text = re.sub( user_regex, user_repl, text)\n",
    "    text = re.sub( url_regex, url_repl, text )\n",
    "    \n",
    "    text = text.replace('\\'','')\n",
    "    \n",
    "    text = re.sub( word_bound_regex , punctuations_repl, text )\n",
    "    text = re.sub( rpt_regex, rpt_repl, text )\n",
    "    \n",
    "    text = sb_stem(text)    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aasd',\n",
       " '__PUNC_EXCL',\n",
       " '__PUNC_QUES',\n",
       " '__user_HOLA',\n",
       " '__HASH_AEAMANITO',\n",
       " '__URL_',\n",
       " 'asd']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d=\"Aasda!.? @Hola #AeaManito https://facebo.com asd\"\n",
    "processAll(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Procesamiento de tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_tweets['processed_tweet'] = processed_tweets.tweetText.apply(processAll)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweetID</th>\n",
       "      <th>tweetText</th>\n",
       "      <th>polarity_value</th>\n",
       "      <th>set</th>\n",
       "      <th>processed_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3112</th>\n",
       "      <td>ID:179860484135206912</td>\n",
       "      <td>El lunes 26, estreno de El Número Uno http://t...</td>\n",
       "      <td>P</td>\n",
       "      <td>train</td>\n",
       "      <td>[lun, estren, numer, uno, __URL_, via, __user_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1266</th>\n",
       "      <td>ID:147716117274701825</td>\n",
       "      <td>Que buena noticia. ;) RT @FeerAlvizar: Primer ...</td>\n",
       "      <td>N</td>\n",
       "      <td>train</td>\n",
       "      <td>[que, buen, notici, __user_FEERALVIZAR, prim, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1103</th>\n",
       "      <td>ID:143987608870060032</td>\n",
       "      <td>BUENOS DÍAS PARTY TIME!!!</td>\n",
       "      <td>P</td>\n",
       "      <td>train</td>\n",
       "      <td>[buen, dias, party, tim, __PUNC_EXCL]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1403</th>\n",
       "      <td>ID:149832675094577152</td>\n",
       "      <td>Cómpralo, no te arrepentirás, Nacho “@nachober...</td>\n",
       "      <td>P</td>\n",
       "      <td>train</td>\n",
       "      <td>[compral, no, arrepent, nach, __user_NACHOBERJ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    tweetID  \\\n",
       "3112  ID:179860484135206912   \n",
       "1266  ID:147716117274701825   \n",
       "1103  ID:143987608870060032   \n",
       "1403  ID:149832675094577152   \n",
       "\n",
       "                                              tweetText polarity_value    set  \\\n",
       "3112  El lunes 26, estreno de El Número Uno http://t...              P  train   \n",
       "1266  Que buena noticia. ;) RT @FeerAlvizar: Primer ...              N  train   \n",
       "1103                          BUENOS DÍAS PARTY TIME!!!              P  train   \n",
       "1403  Cómpralo, no te arrepentirás, Nacho “@nachober...              P  train   \n",
       "\n",
       "                                        processed_tweet  \n",
       "3112  [lun, estren, numer, uno, __URL_, via, __user_...  \n",
       "1266  [que, buen, notici, __user_FEERALVIZAR, prim, ...  \n",
       "1103              [buen, dias, party, tim, __PUNC_EXCL]  \n",
       "1403  [compral, no, arrepent, nach, __user_NACHOBERJ...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_tweets.sample(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emoticones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emoticons\n",
    "emoticons = \\\n",
    "\t[\t# For __EMOT_SMILEY\n",
    "        (' __emoji: U+1F601',\t[':-)', ':)', '(:', '(-:', ] )\t,\\\n",
    "        # for __EMOT_LAUGH\n",
    "\t\t(' __emoji: U+1F923',\t\t[':-D', ':D', 'X-D', 'XD', 'xD', ] )\t,\\\n",
    "        # For __EMOT_LOVE\n",
    "\t\t(' __emoji: U+2764',\t\t['<3', ':\\*', ] )\t,\\\n",
    "        # For __EMOT_WINK\n",
    "\t\t('__emoji: U+1F609',\t\t[';-)', ';)', ';-D', ';D', '(;', '(-;', ] )\t,\\\n",
    "        # For __EMOT_FROWN\n",
    "\t\t(' __emoji: U+2639',\t\t[':-(', ':(', '(:', '(-:', ] )\t,\\\n",
    "        # For __EMOT_CRY\n",
    "\t\t(' __emoji: U+1F622',\t\t[':,(', ':\\'(', ':\"(', ':(('] )\t,\\\n",
    "\t]\n",
    "    \n",
    "def escape_paren(arr):\n",
    "\treturn [text.replace(')', '[)}\\]]').replace('(', '[({\\[]') for text in arr]\n",
    "\n",
    "def regex_union(arr):\n",
    "\treturn '(' + '|'.join( arr ) + ')'\n",
    "\n",
    "emoticons_regex = [ (repl, re.compile(regex_union(escape_paren(regx))) ) for (repl, regx) in emoticons ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a text with one emoticon   __emoji: U+1F601  and another   __emoji: U+2639 \n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "text = \"This is a text with one emoticon :) and another :(\"\n",
    "for (repl, regx) in emoticons_regex :\n",
    "    text = re.sub(regx, ' '+repl+' ', text)\n",
    "    \n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emoji_category</th>\n",
       "      <th>number</th>\n",
       "      <th>code</th>\n",
       "      <th>CLDR_Short_Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>face-positive</td>\n",
       "      <td>1</td>\n",
       "      <td>U+1F600</td>\n",
       "      <td>grinning face</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>face-positive</td>\n",
       "      <td>2</td>\n",
       "      <td>U+1F601</td>\n",
       "      <td>beaming face with smiling eyes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>face-positive</td>\n",
       "      <td>3</td>\n",
       "      <td>U+1F602</td>\n",
       "      <td>face with tears of joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>face-positive</td>\n",
       "      <td>4</td>\n",
       "      <td>U+1F923</td>\n",
       "      <td>rolling on the floor laughing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>face-positive</td>\n",
       "      <td>5</td>\n",
       "      <td>U+1F603</td>\n",
       "      <td>grinning face with big eyes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  emoji_category  number     code                 CLDR_Short_Name\n",
       "0  face-positive       1  U+1F600                   grinning face\n",
       "1  face-positive       2  U+1F601  beaming face with smiling eyes\n",
       "2  face-positive       3  U+1F602          face with tears of joy\n",
       "3  face-positive       4  U+1F923   rolling on the floor laughing\n",
       "4  face-positive       5  U+1F603     grinning face with big eyes"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emojis_db=pd.read_csv('emojis_db_csv.csv')\n",
    "emojis_db.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emojis_unicode(tweet):\n",
    "    ''' Extracts the emojis on the tweet on Unicode format, also tries to match those in regular format, such as \";)\" '''\n",
    "    for (repl, regx) in emoticons_regex :\n",
    "        tweet = re.sub(regx, ' '+repl+' ', tweet)\n",
    "#     print(tweet)\n",
    "\n",
    "    tweet_unicode = str(tweet.encode('unicode-escape'))\n",
    "    tweet_unicode = tweet_unicode.replace('\\\\\\\\U000',' __emoji: U+')\n",
    "#     print(tweet_unicode)\n",
    "    \n",
    "    emoji_list = []\n",
    "#     print(tweet)\n",
    "    for emoji in range(tweet_unicode.count(' __emoji: ')):\n",
    "        em = tweet_unicode.split('__emoji: ')[emoji+1].split()[0]\n",
    "        em = em[:7] # the len of the emoji in unicode is between 6 and 7\n",
    "        emoji_list.append(em.upper())\n",
    "    return emoji_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['U+1F621', 'U+1F923', 'U+1F602']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test\n",
    "text = \"😡🤣😂\"\n",
    "emojis_unicode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['face-negative', 'face-positive', 'face-positive']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def emoji_category(emojis):\n",
    "    categories = []\n",
    "    for i in range(len(emojis)):\n",
    "        # print(emojis[i])\n",
    "        try:\n",
    "            categories.append(emojis_db.loc[emojis_db.code == emojis[i]].emoji_category.values[0])\n",
    "        except:\n",
    "            try:\n",
    "                _ = emojis[i].split('+')[1] + '+'\n",
    "                categories.append(emojis_db[emojis_db.code.str.contains(_)].emoji_category.values[0])\n",
    "            except:\n",
    "                categories.append('other')\n",
    "    if len(categories) < 1:\n",
    "        categories.append('no_emojis')\n",
    "    return categories\n",
    "\n",
    "emoji_category(emojis_unicode(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N- GRAMAS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Unigrams\n",
    "unigrams_fd = nltk.FreqDist()\n",
    "# unigrams_fd.update(text)\n",
    "# unigrams_fd\n",
    "\n",
    "# Bigrams\n",
    "# words_bi  = [ ','.join(map(str,bg)) for bg in nltk.bigrams(text) ]\n",
    "bi_grams_fd = nltk.FreqDist()\n",
    "# bi_grams_fd.update( words_bi )\n",
    "# bi_grams_fd\n",
    "\n",
    "# Trigrams\n",
    "# words_tri  = [ ','.join(map(str,tg)) for tg in nltk.trigrams(text) ]\n",
    "tri_grams_fd = nltk.FreqDist()\n",
    "# tri_grams_fd.update( words_tri )\n",
    "# tri_grams_fd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Wrapper function that encloses all the n-grams procedures\n",
    "def get_word_features(words):\n",
    "    bag = {}\n",
    "    words_uni = [ 'has(%s)'% ug for ug in words ]\n",
    "    words_bi  = [ 'has(%s)'% ','.join(map(str,bg)) for bg in nltk.bigrams(words) ]\n",
    "    words_tri = [ 'has(%s)'% ','.join(map(str,tg)) for tg in nltk.trigrams(words) ]\n",
    "    \n",
    "    for f in words_uni+words_bi+words_tri:\n",
    "        bag[f] = 1\n",
    "\n",
    "    return bag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Negacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "negtn_regex = re.compile( r\"\"\"(?:\n",
    "    ^(?:nunca|no|nada|ningún|ninguno|ninguna|tampoco|\n",
    "        nunc|nad|ningun|tampoc\n",
    "    )$\n",
    ")\n",
    "|\n",
    "n't\n",
    "\"\"\", re.X)\n",
    "\n",
    "def get_negation_features(words):\n",
    "    INF = 0.0\n",
    "    negtn = [ bool(negtn_regex.search(w)) for w in words ]\n",
    "\n",
    "    left = [0.0] * len(words)\n",
    "    prev = 0.0\n",
    "    for i in range(0,len(words)):\n",
    "        if( negtn[i] ):\n",
    "            prev = 1.0\n",
    "        left[i] = prev\n",
    "        prev = max( 0.0, prev-0.1)\n",
    "\n",
    "    right = [0.0] * len(words)\n",
    "    prev = 0.0\n",
    "    for i in reversed(range(0,len(words))):\n",
    "        if( negtn[i] ):\n",
    "            prev = 1.0\n",
    "        right[i] = prev\n",
    "        prev = max( 0.0, prev-0.1)\n",
    "\n",
    "    return dict( zip(\n",
    "                    ['neg_l('+w+')' for w in  words] + ['neg_r('+w+')' for w in  words],\n",
    "                    left + right ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg_l(Este)': 0.0,\n",
       " 'neg_l(tweet)': 0.0,\n",
       " 'neg_l(no)': 1.0,\n",
       " 'neg_l(es)': 0.9,\n",
       " 'neg_l(positivo)': 0.8,\n",
       " 'neg_r(Este)': 0.8,\n",
       " 'neg_r(tweet)': 0.9,\n",
       " 'neg_r(no)': 1.0,\n",
       " 'neg_r(es)': 0.0,\n",
       " 'neg_r(positivo)': 0.0}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test\n",
    "text = [\"Este\",\"tweet\", \"no\", \"es\", \"positivo\"]\n",
    "get_negation_features(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>pos</th>\n",
       "      <th>polarity</th>\n",
       "      <th>xx</th>\n",
       "      <th>word</th>\n",
       "      <th>xxx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>unknown</td>\n",
       "      <td>n</td>\n",
       "      <td>positive</td>\n",
       "      <td>1.0</td>\n",
       "      <td>felicitación</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>unknown</td>\n",
       "      <td>a</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0</td>\n",
       "      <td>inconsciente</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>unknown</td>\n",
       "      <td>n</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1.0</td>\n",
       "      <td>soporte</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>unknown</td>\n",
       "      <td>n</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1.0</td>\n",
       "      <td>proceso_matemático</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>unknown</td>\n",
       "      <td>v</td>\n",
       "      <td>positive</td>\n",
       "      <td>1.0</td>\n",
       "      <td>manifestar</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         x pos  polarity   xx                word  xxx\n",
       "0  unknown   n  positive  1.0        felicitación    1\n",
       "1  unknown   a  negative  1.0        inconsciente    1\n",
       "2  unknown   n   neutral  1.0             soporte    1\n",
       "3  unknown   n   neutral  1.0  proceso_matemático    1\n",
       "4  unknown   v  positive  1.0          manifestar    1"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opener_lexicon = pd.read_csv('https://raw.githubusercontent.com/opener-project/public-sentiment-lexicons/master/propagation_lexicons/es/es.lemma.sy.an.hypo.rels.maxdepth5.seed500.maj.gold.csv', sep=';', header=None)\n",
    "opener_lexicon.columns = ['x','pos', 'polarity', 'xx','word','xxx']\n",
    "opener_lexicon.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sent(polarity):\n",
    "    if polarity == 'neutral':\n",
    "        return 0\n",
    "    if polarity == 'negative':\n",
    "        return -1\n",
    "    if polarity == 'positive':\n",
    "        return 1\n",
    "opener_lexicon['sentiment'] = opener_lexicon.polarity.apply(get_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>stmd_word</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8143</th>\n",
       "      <td>cofirmar</td>\n",
       "      <td>cofirm</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2804</th>\n",
       "      <td>investigador_privado</td>\n",
       "      <td>investigador_priv</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4107</th>\n",
       "      <td>incrementar_la_intensidad</td>\n",
       "      <td>incrementar_la_intens</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10594</th>\n",
       "      <td>defensividad</td>\n",
       "      <td>defens</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            word              stmd_word  sentiment\n",
       "8143                    cofirmar                 cofirm        1.0\n",
       "2804        investigador_privado      investigador_priv        1.0\n",
       "4107   incrementar_la_intensidad  incrementar_la_intens        1.0\n",
       "10594               defensividad                 defens        1.0"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opener_lexicon['stmd_word'] = opener_lexicon['word'].apply(sb_stem)\n",
    "opener_lexicon['stmd_word'] = opener_lexicon.stmd_word.apply(lambda x: str(x).replace('[','').replace(']','').replace(\"'\",''))\n",
    "opener_lexicon = opener_lexicon[['word','stmd_word','sentiment']]\n",
    "opener_lexicon = opener_lexicon.loc[opener_lexicon.sentiment != 0]\n",
    "opener_lexicon = opener_lexicon.drop_duplicates()\n",
    "opener_lexicon.sample(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet  ajá! “@JulenAriza: mal síntoma, especialmente para Newt Gingrich, que no ha podido conseguir 10.000 firmas en el mismo estado donde vive.”\n",
      "Would have the following words and sentiments associated:\n",
      "{'sent(mal)': -1.0, 'sent(especial)': 1.0, 'sent(pod)': -1.0, 'sent(consegu)': 1.0, 'sent(firm)': 1.0}\n"
     ]
    }
   ],
   "source": [
    "def get_polarity_features(text):\n",
    "    pol = {}\n",
    "    text = sb_stem(text)\n",
    "    for word in text:\n",
    "        try:\n",
    "            pol[\"sent(\"+word+\")\"] = opener_lexicon.loc[opener_lexicon.stmd_word == word].sentiment.values[0]\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "    return pol\n",
    "\n",
    "_ = processed_tweets.tweetText[1503]\n",
    "print('Tweet ',_)\n",
    "print('Would have the following words and sentiments associated:')\n",
    "print(get_polarity_features(_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aplicando los features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapper function for the extraction of features\n",
    "def extract_features(text):\n",
    "    global usr_names, hstgs\n",
    "    features = {}\n",
    "    words = text\n",
    "    words = processAll(text)\n",
    "\n",
    "    word_features = get_word_features(words)\n",
    "    features.update( word_features )\n",
    "\n",
    "    negation_features = get_negation_features(words)\n",
    "    features.update( negation_features )\n",
    "    \n",
    "    # Sentiment features are not included on the final deliverabe as did not improve results\n",
    "#     sentiment_features = get_polarity_features(text)\n",
    "#     features.update(sentiment_features )\n",
    "    \n",
    "    emoji_features = emoji_category(emojis_unicode(text))\n",
    "    emoji_features_dic = dict( zip(['emoji_('+w+')' for w in  emoji_features], emoji_features))\n",
    "    features.update( emoji_features_dic )\n",
    "    \n",
    "    usr_names = list(set(usr_names))\n",
    "    hstgs = list(set(hstgs))\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_tweets['processed_tweet_features'] = processed_tweets.tweetText.apply(extract_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'has(dej)': 1,\n",
       " 'has(que)': 1,\n",
       " 'has(hic)': 1,\n",
       " 'has(sea)': 1,\n",
       " 'has(olvid)': 1,\n",
       " 'has(__HASH_MERINONOMEREPRESENTA)': 1,\n",
       " 'has(__HASH_NUEVACONSTITUCION)': 1,\n",
       " 'has(__URL_)': 1,\n",
       " 'has(dej,que)': 1,\n",
       " 'has(que,que)': 1,\n",
       " 'has(que,hic)': 1,\n",
       " 'has(hic,sea)': 1,\n",
       " 'has(sea,olvid)': 1,\n",
       " 'has(olvid,__HASH_MERINONOMEREPRESENTA)': 1,\n",
       " 'has(__HASH_MERINONOMEREPRESENTA,__HASH_NUEVACONSTITUCION)': 1,\n",
       " 'has(__HASH_NUEVACONSTITUCION,__URL_)': 1,\n",
       " 'has(dej,que,que)': 1,\n",
       " 'has(que,que,hic)': 1,\n",
       " 'has(que,hic,sea)': 1,\n",
       " 'has(hic,sea,olvid)': 1,\n",
       " 'has(sea,olvid,__HASH_MERINONOMEREPRESENTA)': 1,\n",
       " 'has(olvid,__HASH_MERINONOMEREPRESENTA,__HASH_NUEVACONSTITUCION)': 1,\n",
       " 'has(__HASH_MERINONOMEREPRESENTA,__HASH_NUEVACONSTITUCION,__URL_)': 1,\n",
       " 'neg_l(dej)': 0.0,\n",
       " 'neg_l(que)': 0.0,\n",
       " 'neg_l(hic)': 0.0,\n",
       " 'neg_l(sea)': 0.0,\n",
       " 'neg_l(olvid)': 0.0,\n",
       " 'neg_l(__HASH_MERINONOMEREPRESENTA)': 0.0,\n",
       " 'neg_l(__HASH_NUEVACONSTITUCION)': 0.0,\n",
       " 'neg_l(__URL_)': 0.0,\n",
       " 'neg_r(dej)': 0.0,\n",
       " 'neg_r(que)': 0.0,\n",
       " 'neg_r(hic)': 0.0,\n",
       " 'neg_r(sea)': 0.0,\n",
       " 'neg_r(olvid)': 0.0,\n",
       " 'neg_r(__HASH_MERINONOMEREPRESENTA)': 0.0,\n",
       " 'neg_r(__HASH_NUEVACONSTITUCION)': 0.0,\n",
       " 'neg_r(__URL_)': 0.0,\n",
       " 'emoji_(no_emojis)': 'no_emojis'}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_tweets.processed_tweet_features[150]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parte 1 : Clasificacion - Analisis de Sentimientos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ = processed_tweets.loc[processed_tweets.set == 'train']\n",
    "test_ = processed_tweets.loc[processed_tweets.set == 'test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweetID</th>\n",
       "      <th>tweetText</th>\n",
       "      <th>polarity_value</th>\n",
       "      <th>set</th>\n",
       "      <th>processed_tweet</th>\n",
       "      <th>processed_tweet_features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1341206151204106241</td>\n",
       "      <td>ME PREGUNTO SI LOS QUE ESCRIBEN #ContraTodoYCo...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "      <td>[pregunt, los, que, escrib, __HASH_CONTRATODOY...</td>\n",
       "      <td>{'has(pregunt)': 1, 'has(los)': 1, 'has(que)':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1341031364569812992</td>\n",
       "      <td>@Cycl0p77 @patriciagamarra Si llevan un cartel...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "      <td>[__user_CYCL0P77, __user_PATRICIAGAMARRA, llev...</td>\n",
       "      <td>{'has(__user_CYCL0P77)': 1, 'has(__user_PATRIC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1340782914250928135</td>\n",
       "      <td>@marcogiovannic #MerinoNoMeRepresenta</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "      <td>[__user_MARCOGIOVANNIC, __HASH_MERINONOMEREPRE...</td>\n",
       "      <td>{'has(__user_MARCOGIOVANNIC)': 1, 'has(__HASH_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1340649911495372800</td>\n",
       "      <td>@otravezandres @SomosElComercio Están en la pl...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "      <td>[__user_OTRAVEZANDRES, __user_SOMOSELCOMERCIO,...</td>\n",
       "      <td>{'has(__user_OTRAVEZANDRES)': 1, 'has(__user_S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1340501298681147392</td>\n",
       "      <td>#nofilter #natural #lovechristmas #lovemyself ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "      <td>[__HASH_NOFILTER, __HASH_NATURAL, __HASH_LOVEC...</td>\n",
       "      <td>{'has(__HASH_NOFILTER)': 1, 'has(__HASH_NATURA...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               tweetID                                          tweetText  \\\n",
       "0  1341206151204106241  ME PREGUNTO SI LOS QUE ESCRIBEN #ContraTodoYCo...   \n",
       "1  1341031364569812992  @Cycl0p77 @patriciagamarra Si llevan un cartel...   \n",
       "2  1340782914250928135              @marcogiovannic #MerinoNoMeRepresenta   \n",
       "3  1340649911495372800  @otravezandres @SomosElComercio Están en la pl...   \n",
       "4  1340501298681147392  #nofilter #natural #lovechristmas #lovemyself ...   \n",
       "\n",
       "  polarity_value   set                                    processed_tweet  \\\n",
       "0            NaN  test  [pregunt, los, que, escrib, __HASH_CONTRATODOY...   \n",
       "1            NaN  test  [__user_CYCL0P77, __user_PATRICIAGAMARRA, llev...   \n",
       "2            NaN  test  [__user_MARCOGIOVANNIC, __HASH_MERINONOMEREPRE...   \n",
       "3            NaN  test  [__user_OTRAVEZANDRES, __user_SOMOSELCOMERCIO,...   \n",
       "4            NaN  test  [__HASH_NOFILTER, __HASH_NATURAL, __HASH_LOVEC...   \n",
       "\n",
       "                            processed_tweet_features  \n",
       "0  {'has(pregunt)': 1, 'has(los)': 1, 'has(que)':...  \n",
       "1  {'has(__user_CYCL0P77)': 1, 'has(__user_PATRIC...  \n",
       "2  {'has(__user_MARCOGIOVANNIC)': 1, 'has(__HASH_...  \n",
       "3  {'has(__user_OTRAVEZANDRES)': 1, 'has(__user_S...  \n",
       "4  {'has(__HASH_NOFILTER)': 1, 'has(__HASH_NATURA...  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
